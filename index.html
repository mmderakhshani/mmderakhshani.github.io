<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Mohammad M. Derakhshani</title>

  <meta name="author" content="Mohammad Mahdi Derakhshani">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <main class="main">
    <article>
      <h1>
        Mohammad Mahdi Derakhshani
      </h1>
      <p> Computer Vision | Machine Learning </p>
      <p>
        Hi! I'm Mohammad, a Ph.D. student of <a href="https://ivi.fnwi.uva.nl/vislab/">VIS lab</a> at the
        University of Amsterdam working with <a href="https://scholar.google.com/citations?user=0uKdbscAAAAJ&hl=en">Cees
          Snoek</a> and <a href="https://scholar.google.co.uk/citations?user=CdpLhlgAAAAJ&hl=en">Yuki Asano</a>. My
        Ph.D.
        thesis is <strong>Efficient Adaptation of Large-scale Vision and Language Models</strong>.
        Specifically, my current focus is on the implementation of prompting and chain of thoughts for
        multi-modal reasoning.
        <br>
        <br>
        In summer 2022, I was also a ML/CV intern at Samsung AI Center (SAIC) in Cambridge working with <a
          href="https://scholar.google.co.uk/citations?user=-62MApgAAAAJ&hl=en">Brais Martinez</a> and <a
          href="https://scholar.google.co.uk/citations?user=D4JkWxf-8fwC&hl=en">Georgios Tzimiropoulos</a>
        doing research on <strong>Prompting of large-scale language-image models</strong> and
        <strong>Federated Learning</strong>.
        <br>
        <br>
        Previously I was a master student at the University of Tehran working with <a
          href="https://scholar.google.com/citations?user=FTcata0AAAAJ&hl=en">Babak Nadjar Araabi</a> and <a
          href="https://scholar.google.com/citations?user=Viogmi8AAAAJ&hl=en">Mohammad Amin Sadeghi</a>. I
        did my master at machine learning and computational modeling lab working on object detection and
        image compression. During my master degree, I also did research on object detection with <a
          href="https://scholar.google.com/citations?user=N4-2Z_cAAAAJ&hl=en">Mohammad Rastegari</a> at
        Allen AI Institute.
        <br>
        <br>
        I am a member of <a href="https://ellis.eu/projects/continual-learning">ELLIS society</a>, and I
        have served as reviewer for ICCV'21, ICML'22, CVPR'23, ICLR'23, and TPAMI.
      </p>
      <div>
        <a href="mailto:m.m.derakhshani@uva.nl">Email</a> &nbsp/&nbsp
        <a href="https://scholar.google.com/citations?user=n7GnOJoAAAAJ&hl=en">Google Scholar</a>
        &nbsp/&nbsp
        <a href="https://github.com/mmderakhshani">Github</a>&nbsp/&nbsp
        <a href="https://twitter.com/mmderakhshani">Twitter</a>&nbsp/&nbsp
        <a href="https://www.linkedin.com/in/mmderakhshani/">LinkedIn</a>&nbsp/&nbsp
        <a href="data/MohammadMahdiDerakhshaniCV.pdf">CV</a>
      </div>
      <a href="images/mohammad.png"><img class="portfolio-image" alt="profile photo" src="images/mohammad.png" class="hoverZoomLink"></a>
    </article>

    <section>
      <h2>News</h2>
      <p>
      <ul>
        <li>TA for Deep Learning 2022 by <a href="https://yukimasano.github.io/">Yuki Asano</a>.</li>
        <li>New preprint: <a href=https://arxiv.org/pdf/2210.02390.pdf>variational prompt tuning</a>
          improves generalization of vision and language models.</li>
        <li>Joined Samsung AI Center (SAIC) as CV/ML summer intern in Cambridge in UK.</li>
        <li>One paper accepted to MICCAI'22: introducing <a href="https://arxiv.org/abs/2204.05737">LifeLonger</a>, a
          continual learning benchmark for
          disease classification.</li>
        <li>TA for Deep Learning 2021 by <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong
            Zhen</a>.</li>
        <li>New preprint: introducing <a href="https://arxiv.org/abs/2112.13410">generative kernel continual
            learning</a>.</li>
        <li>One paper accepted to ICML'21: introducing <a href="https://arxiv.org/abs/2107.05757">kernel
            continual learning</a>.</li>
        <li>TA for Machine Learning 2020 by <a href="https://enalisnick.github.io/">Eric Nalisnick</a>.</li>
        <li>Staring my PhD in January 2020</li>
        <li>One paper accepted to CVPR'19: introducing <a href="https://arxiv.org/abs/1906.05388">Assisted
            Excitation of Activations</a> for object detection.</li>
        <li>One paper accepted to CVPR'18 (Workshop): introducing <a href="https://arxiv.org/abs/1805.11091">
            BlockCNN</a> for artifact removal and image
          compression.</li>
      </ul>
      </p>
    </section>

    <section>
      <h2>Research</h2>
      <p>
        I'm interested in computer vision and multi-modal learning (vision & language models), specifically
        the prompting and chain of thoughts for multi-modal reasoning.
      </p>
    </section>

    <section>
      <img src="images/variational_prompt.png" alt="method" width="160" height="100">
      <a href="https://arxiv.org/abs/2210.02390">
        <p>Variational prompt tuning improves generalization of vision-language models</p>
      </a>
      <br>
      <strong>Mohammad Mahdi Derakhshani</strong>, Enrique Sanchez, Adrian Bulat, Victor Guilherme Turrisi
      da Costa, Cees G. M. Snoek, Georgios Tzimiropoulos, Brais Martinez.
      <br>
      <em>arXiv</em> 2022
      <br>
      <a href="data/derakhshani2022variational.bib">bibtex</a>
      <p> We propose a probabilistic modeling of the underlying distribution of prompts, allowing prompts
        within the support of an associated concept to be derived through stochastic sampling. This results
        in a more complete and richer transfer of the information captured by the language model, providing
        better generalization capabilities for downstream tasks.</p>
      <img src="images/task-class-cross-domain-il.png" alt="method" width="160" height="100">
      <a href="https://arxiv.org/abs/2204.05737">
        <p>LifeLonger: A Benchmark for Continual Disease Classification</p>
      </a>
      <br>
      <strong>Mohammad Mahdi Derakhshani</strong>, Ivona Najdenkoska, Tom van Sonsbeek, Xiantong Zhen,
      Dwarikanath Mahapatra, Marcel Worring, Cees G. M. Snoek.
      <br>
      <em>MICCAI</em> 2022
      <br>
      <a href="data/derakhshani2022lifelonger.bib">bibtex</a>
      <p> We introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST
        collection, by applying existing state-of-the-art continual learning methods. We perform a thorough
        analysis of the performance and examine how the well-known challenges of continual learning, such as
        the catastrophic forgetting exhibit themselves in this setting.</p>
      <img src="images/generative_kcl.png" alt="method" width="160" height="100">
      <a href="https://arxiv.org/abs/2112.13410">
        <p>Generative Kernel Continual learning</p>
      </a>
      <br>
      <strong>Mohammad Mahdi Derakhshani</strong>, Xiantong Zhen, Ling Shao, Cees G. M. Snoek
      <br>
      <em>arXiv</em> 2021
      <br>
      <a href="data/derakhshani2021generative.bib">bibtex</a>
      <p> We introduce generative kernel continual learning, which explores and exploits the synergies
        between generative models and kernels for continual learning.</p>


      <img src="images/kcl.png" alt="method" width="160" height="100">
      <a href="https://arxiv.org/abs/2210.02390">
        <p>Kernel Continual learning</p>
      </a>
      <br>
      <strong>Mohammad Mahdi Derakhshani</strong>, Xiantong Zhen, Ling Shao, Cees G. M. Snoek
      <br>
      <em>ICML</em> 2021
      <br>
      <a href="data/derakhshani2021kernel.bib">bibtex</a>
      <p> This paper introduces kernel continual learning, a simple but effective variant of continual
        learning that leverages the non-parametric nature of kernel methods to tackle catastrophic
        forgetting.</p>

      <img src="images/assisted.png" alt="method" width="160" height="100">

      <a href="https://arxiv.org/abs/1906.05388">
        <p>Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors
        </p>
      </a>
      <br>
      <strong>Mohammad Mahdi Derakhshani</strong>, Saeed Masoudnia, Amir Hossein Shaker, Omid Mersa,
      Mohammad Amin Sadeghi, Mohammad Rastegari, Babak N. Araabi
      <br>
      <em>CVPR</em> 2019
      <br>
      <a href="data/derakhshani2019assisted.bib">bibtex</a>
      <p> We present a simple and effective learning technique that significantly improves mAP of YOLO
        object detectors without compromising their speed.</p>
      <img src="images/blockcnn.png" alt="method" width="160" height="100">
      <a href="https://arxiv.org/abs/2210.02390">
        <p>BlockCNN: A Deep Network for Artifact Removal and Image Compression</p>
      </a>
      <br>
      Danial Maleki, Soheila Nadalian, Mohammad Mahdi Derakhshani, <strong>Mohammad Mahdi
        Derakhshani</strong>, Mohammad Amin Sadeghi
      <br>
      <em>CVPR (Workshop)</em> 2018
      <br>
      <a href="data/maleki2018blockcnn.bib">bibtex</a>
      <p> We present a general technique that performs both artifact removal and image compression. For
        artifact removal, we input a JPEG image and try to remove its compression artifacts.</p>
    </section>

    <footer class="footer">
      <p>
        Great template from <a href="https://github.com/mmderakhshani/mmderakhshani.github.io" target="_blank">
          Mohammad Mahdi
          Derakhshani
        </a>
      </p>
    </footer>
  </main>
</body>

</html>