<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Mohammad M. Derakhshani</title>

  <meta name="author" content="Mohammad Mahdi Derakhshani">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
</head>

<body>
  <main class="main">
    <article class="intro">
      <div class="intro__info">
        <h1>
          Mohammad Mahdi Derakhshani
        </h1>
        <p class="intro__skills"> Computer Vision | Machine Learning </p>
        <ul class="socials">
          <li class="socials__item"><a target="_blank" href="mailto:m.m.derakhshani@uva.nl">Email</a></li>
          <li class="socials__item"><a target="_blank"
              href="https://scholar.google.com/citations?user=n7GnOJoAAAAJ&hl=en">Google
              Scholar</a></li>
          <li class="socials__item"><a target="_blank" href="https://github.com/mmderakhshani">Github</a></li>
          <li class="socials__item"><a target="_blank" href="https://twitter.com/mmderakhshani">Twitter</a></li>
          <li class="socials__item"><a target="_blank" href="https://www.linkedin.com/in/mmderakhshani/">LinkedIn</a>
          </li>
          <li class="socials__item"><a target="_blank" href="data/MohammadMahdiDerakhshaniCV.pdf">CV</a></li>
        </ul>
        <p class="intro__about-me">
          Hi! I'm Mohammad, a Ph.D. student of <a target="_blank" href="https://ivi.fnwi.uva.nl/vislab/">VIS lab</a> at
          the
          University of Amsterdam working with <a target="_blank"
            href="https://scholar.google.com/citations?user=0uKdbscAAAAJ&hl=en">Cees
            Snoek</a> and <a target="_blank" href="https://scholar.google.co.uk/citations?user=CdpLhlgAAAAJ&hl=en">Yuki
            Asano</a>. My
          Ph.D.
          thesis is <strong>Efficient Adaptation of Large-scale Vision and Language Models</strong>.
          Specifically, my current focus is on the implementation of prompting and chain of thoughts for
          multi-modal reasoning.
          <br>
          <br>
          In summer 2022, I was also a ML/CV intern at Samsung AI Center (SAIC) in Cambridge working with <a
            target="_blank" href="https://scholar.google.co.uk/citations?user=-62MApgAAAAJ&hl=en">Brais Martinez</a> and
          <a target="_blank" href="https://scholar.google.co.uk/citations?user=D4JkWxf-8fwC&hl=en">Georgios
            Tzimiropoulos</a>
          doing research on <strong>Prompting of large-scale language-image models</strong> and
          <strong>Federated Learning</strong>.
          <br>
          <br>
          Previously I was a master student at the University of Tehran working with <a target="_blank"
            href="https://scholar.google.com/citations?user=FTcata0AAAAJ&hl=en">Babak Nadjar Araabi</a> and <a
            target="_blank" href="https://scholar.google.com/citations?user=Viogmi8AAAAJ&hl=en">Mohammad Amin
            Sadeghi</a>. I
          did my master at machine learning and computational modeling lab working on object detection and
          image compression. During my master degree, I also did research on object detection with <a target="_blank"
            href="https://scholar.google.com/citations?user=N4-2Z_cAAAAJ&hl=en">Mohammad Rastegari</a> at
          Allen AI Institute.
          <br>
          <br>
          I am a member of <a target="_blank" href="https://ellis.eu/projects/continual-learning">ELLIS society</a>, and
          I
          have served as reviewer for ICCV'21, ICML'22, CVPR'23, ICLR'23, and TPAMI.
        </p>
      </div>
      <div class="intro__image">
        <img class="portfolio-image" alt="profile photo" src="images/mohammad.png">
      </div>
    </article>

    <section class="section">
      <h2>News</h2>
      <ul class="news-list">
        <li>TA for Deep Learning 2022 by <a target="_blank" href="https://yukimasano.github.io/">Yuki Asano</a>.</li>
        <li>New preprint: <a target="_blank" href=https://arxiv.org/pdf/2210.02390.pdf>variational prompt tuning</a>
          improves generalization of vision and language models.</li>
        <li>Joined Samsung AI Center (SAIC) as CV/ML summer intern in Cambridge in UK.</li>
        <li>One paper accepted to MICCAI'22: introducing <a target="_blank"
            href="https://arxiv.org/abs/2204.05737">LifeLonger</a>, a
          continual learning benchmark for
          disease classification.</li>
        <li>TA for Deep Learning 2021 by <a target="_blank"
            href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong
            Zhen</a>.</li>
        <li>New preprint: introducing <a target="_blank" href="https://arxiv.org/abs/2112.13410">generative kernel
            continual
            learning</a>.</li>
        <li>One paper accepted to ICML'21: introducing <a target="_blank" href="https://arxiv.org/abs/2107.05757">kernel
            continual learning</a>.</li>
        <li>TA for Machine Learning 2020 by <a target="_blank" href="https://enalisnick.github.io/">Eric Nalisnick</a>.
        </li>
        <li>Staring my PhD in January 2020</li>
        <li>One paper accepted to CVPR'19: introducing <a target="_blank"
            href="https://arxiv.org/abs/1906.05388">Assisted
            Excitation of Activations</a> for object detection.</li>
        <li>One paper accepted to CVPR'18 (Workshop): introducing <a target="_blank"
            href="https://arxiv.org/abs/1805.11091">
            BlockCNN</a> for artifact removal and image
          compression.</li>
      </ul>
    </section>

    <section class="section">
      <h2>Research</h2>
      <p>
        I'm interested in computer vision and multi-modal learning (vision & language models), specifically
        the prompting and chain of thoughts for multi-modal reasoning.
      </p>
    </section>

    <section>
      <div class="research">
        <img class="research__img" src="images/variational_prompt.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2210.02390">
            Variational prompt tuning improves generalization of vision-language models
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Enrique Sanchez, Adrian Bulat, Victor Guilherme Turrisi
            da Costa, Cees G. M. Snoek, Georgios Tzimiropoulos, Brais Martinez.</p>
          <em>arXiv</em> <time>2022</time>
          <a target="_blank" href="data/derakhshani2022variational.bib">bibtex</a>
          <p> We propose a probabilistic modeling of the underlying distribution of prompts, allowing prompts
            within the support of an associated concept to be derived through stochastic sampling. This results
            in a more complete and richer transfer of the information captured by the language model, providing
            better generalization capabilities for downstream tasks.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/task-class-cross-domain-il.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2204.05737">
            LifeLonger: A Benchmark for Continual Disease Classification
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Ivona Najdenkoska, Tom van Sonsbeek, Xiantong Zhen,
            Dwarikanath Mahapatra, Marcel Worring, Cees G. M. Snoek.</p>
          <em>MICCAI</em> <time>2022</time>
          <a target="_blank" href="data/derakhshani2022lifelonger.bib">bibtex</a>
          <p> We introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST
            collection, by applying existing state-of-the-art continual learning methods. We perform a thorough
            analysis of the performance and examine how the well-known challenges of continual learning, such as
            the catastrophic forgetting exhibit themselves in this setting.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/generative_kcl.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2112.13410">
            Generative Kernel Continual learning
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Xiantong Zhen, Ling Shao, Cees G. M. Snoek</p>
          <em>arXiv</em> <time>2021</time>
          <a target="_blank" href="data/derakhshani2021generative.bib">bibtex</a>
          <p> We introduce generative kernel continual learning, which explores and exploits the synergies
            between generative models and kernels for continual learning.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/kcl.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2210.02390">
            Kernel Continual learning
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Xiantong Zhen, Ling Shao, Cees G. M. Snoek</p>
          <em>ICML</em> <time>2021</time>
          <a target="_blank" href="data/derakhshani2021kernel.bib">bibtex</a>
          <p> This paper introduces kernel continual learning, a simple but effective variant of continual
            learning that leverages the non-parametric nature of kernel methods to tackle catastrophic
            forgetting.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" class="research__img" src="images/assisted.png" alt="method" width="160"
          height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/1906.05388">
            Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Saeed Masoudnia, Amir Hossein Shaker, Omid Mersa,
            Mohammad Amin Sadeghi, Mohammad Rastegari, Babak N. Araabi</p>

          <em>CVPR</em> <time>2019</time>
          <a target="_blank" href="data/derakhshani2019assisted.bib">bibtex</a>
          <p> We present a simple and effective learning technique that significantly improves mAP of YOLO
            object detectors without compromising their speed.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/blockcnn.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2210.02390">
            BlockCNN: A Deep Network for Artifact Removal and Image Compression
          </a>
          <p>Danial Maleki, Soheila Nadalian, Mohammad Mahdi Derakhshani, <strong>Mohammad Mahdi
              Derakhshani</strong>, Mohammad Amin Sadeghi</p>
          <em>CVPR (Workshop)</em> <time>2018</time>
          <a target="_blank" href="data/maleki2018blockcnn.bib">bibtex</a>
          <p> We present a general technique that performs both artifact removal and image compression. For
            artifact removal, we input a JPEG image and try to remove its compression artifacts.</p>
        </div>
      </div>
    </section>

    <footer class="footer">
      <p>
        Great template from <a target="_blank" href="https://github.com/mmderakhshani/mmderakhshani.github.io"
          target="_blank">
          Mohammad Mahdi
          Derakhshani
        </a>
      </p>
    </footer>
  </main>
</body>

</html>
