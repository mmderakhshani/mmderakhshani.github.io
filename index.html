<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Mohammad M. Derakhshani</title>

  <meta name="author" content="Mohammad Mahdi Derakhshani">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
</head>

<body>
  <main class="main">
    <article class="intro">
      <div class="intro__info">
        <h1>
          Mohammad Mahdi Derakhshani
        </h1>
        <p class="intro__skills"> Computer Vision | Machine Learning </p>
        <p class="intro__about-me">
          Welcome! I'm Mohammad, a Ph.D. student at the University of Amsterdam's <a target="_blank"
            href="https://ivi.fnwi.uva.nl/vislab/">VIS lab</a> at the
          collaborating with <a target="_blank" href="https://scholar.google.com/citations?user=0uKdbscAAAAJ&hl=en">Cees
            Snoek</a> and <a target="_blank" href="https://scholar.google.co.uk/citations?user=CdpLhlgAAAAJ&hl=en">Yuki
            Asano</a>. My research delves into <strong>multi-modal foundation models</strong>, with a keen interest in
          their architecture and training datasets.


          <br>
          <br>
          In summer 2023, I interned at Microsoft Research, Cambridge, working on fine-tuning large-scale LLMs, e.g.
          GPT3 and GPT3.5 and conditional text-to-image generation
          alongside <a target="_blank" href="https://www.microsoft.com/en-us/research/people/mollyxia/">Molly Xia</a>,
          <a target="_blank" href="https://scholar.google.co.in/citations?user=R7k23-0AAAAJ&hl=en">Harkirat Behl</a> and
          <a target="_blank" href="https://www.microsoft.com/en-us/research/people/virueh/">Victor Ruehle</a>.
          In summer 2022, I was at Samsung AI Center, Cambridge, researching large-scale language-image models and
          Federated Learning with <a target="_blank"
            href="https://scholar.google.co.uk/citations?user=-62MApgAAAAJ&hl=en">Brais Martinez</a> and
          <a target="_blank" href="https://scholar.google.co.uk/citations?user=D4JkWxf-8fwC&hl=en">Georgios
            Tzimiropoulos</a>.
          <br>
          <br>
          Before that, I pursued my master's at the University of Tehran under the guidance of <a target="_blank"
            href="https://scholar.google.com/citations?user=FTcata0AAAAJ&hl=en">Babak Nadjar Araabi</a> and <a
            target="_blank" href="https://scholar.google.com/citations?user=Viogmi8AAAAJ&hl=en">Mohammad Amin
            Sadeghi</a>. My studies at the Machine Learning and Computational Modeling lab focused on object
          detection and image compression. I also researched object detection with <a target="_blank"
            href="https://scholar.google.com/citations?user=N4-2Z_cAAAAJ&hl=en">Mohammad Rastegari</a>.
          <br>
          <br>
          I'm proud to be an <a target="_blank" href="https://ellis.eu/projects/continual-learning">ELLIS society</a>
          member and have reviewed for prestigious conferences such as CVPR, NeurIPs, ICLR, ICML, ICCV and TPAMI.
        <ul class="socials">
          <li class="socials__item"><a target="_blank" href="mailto:m.m.derakhshani@uva.nl">Email</a></li>
          <li class="socials__item"><a target="_blank"
              href="https://scholar.google.com/citations?user=n7GnOJoAAAAJ&hl=en">Google
              Scholar</a></li>
          <li class="socials__item"><a target="_blank" href="https://github.com/mmderakhshani">Github</a></li>
          <li class="socials__item"><a target="_blank" href="https://twitter.com/mmderakhshani">Twitter</a></li>
          <li class="socials__item"><a target="_blank" href="https://www.linkedin.com/in/mmderakhshani/">LinkedIn</a>
          </li>
          <!-- <li class="socials__item"><a target="_blank" href="data/MohammadMahdiDerakhshaniCV.pdf">CV</a></li> -->
        </ul>
        </p>
      </div>
      <div class="intro__image">
        <img class="portfolio-image" alt="profile photo" src="images/mohammad.png">
      </div>
    </article>

    <section class="section">
      <h2>News</h2>
      <ul class="news-list">
        <li>&#128220; Preprint'23: <a target="_blank" href=https://arxiv.org/abs/2310.00500>Small Visual Language Models
            can also be Open-Ended Few-Shot Learners</a></li>
        <li>&#128220; ICCV'23: <a target="_blank" href=https://arxiv.org/pdf/2210.02390.pdf>Bayesian Prompt Learning for
            Image-Language Model Generalization</a></li>
        <li>&#128293; Joined Microsoft Research as ML summer intern in Cambridge in UK.</li>
        <li>&#128220; [Oral] MICCAI'23: <a target="_blank" href=https://arxiv.org/pdf/2303.05977.pdf>Open-Ended Medical
            Visual Question Answering Through Prefix Tuning of Language Models.</a></li>
        <li>&#128220; ICLR'23 - Workshop on ME-FoMo: <a target="_blank"
            href=https://openreview.net/forum?id=lJ5KlMWUzB>Variational prompt tuning improves generalization of
            vision-language foundation models.</a></li>
        <li>&#128293; Joined Samsung AI Center (SAIC) as CV/ML summer intern in Cambridge in UK.</li>
      </ul>
    </section>

    <section class="section">
      <h2>Research</h2>
      <br>
      <br>
    </section>

    <section>
      <div class="research">
        <img class="research__img" src="images/icl_framework_3.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2310.00500">
            Small Visual Language Models can also be Open-Ended Few-Shot Learners
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>*, Ivona Najdenkoska*, Cees G. M. Snoek, Marcel Worring, Yuki M.
            Asano.</p>
          <em>Arxiv</em> <time>2023</time>
          <a target="_blank" href="data/derakhshani2022variational.bib">bibtex</a>
          <p> We present Self-Context Adaptation (SeCAt), a self-supervised approach that unlocks open-ended few-shot
            abilities of small visual language models. Our proposed adaptation algorithm explicitly learns from
            symbolic, yet self-supervised training tasks.
          </p>
        </div>
      </div>


      <div class="research">
        <img class="research__img" src="images/Figure1_karla_2.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2210.02390">
            Bayesian Prompt Learning for Image-Language Model Generalization
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Enrique Sanchez, Adrian Bulat, Victor Guilherme Turrisi
            da Costa, Cees G. M. Snoek, Georgios Tzimiropoulos, Brais Martinez.</p>
          <em>ICCV</em> <time>2023</time>
          <a target="_blank" href="data/derakhshani2022variational.bib">bibtex</a>
          <p> We propose a probabilistic modeling of the underlying distribution of prompts, allowing prompts
            within the support of an associated concept to be derived through stochastic sampling. This results
            in a more complete and richer transfer of the information captured by the language model, providing
            better generalization capabilities for downstream tasks.
          </p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/micca_23.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2303.05977">
            Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models
          </a>
          <p>Tom van Sonsbeek*, <strong>Mohammad Mahdi Derakhshani*</strong>, Ivona Najdenkoska*, Cees G. M. Snoek,
            Marcel Worring.</p>
          <em>MICCAI</em> <time>2023</time>
          <a target="_blank" href="data/van2023open.bib">bibtex</a>
          <p> we introduce a novel method particularly suited for small, domain-specific, medical datasets. To properly
            communicate the medical images to the language model, we develop a network that maps the extracted
            visual features to a set of learnable tokens. Then, alongside the question, these learnable tokens
            directly prompt the language model.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/task-class-cross-domain-il.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2204.05737">
            LifeLonger: A Benchmark for Continual Disease Classification
          </a>
          <p><strong>Mohammad Mahdi Derakhshani*</strong>, Ivona Najdenkoska*, Tom van Sonsbeek*, Xiantong Zhen,
            Dwarikanath Mahapatra, Marcel Worring, Cees G. M. Snoek.</p>
          <em>MICCAI</em> <time>2022</time>
          <a target="_blank" href="data/derakhshani2022lifelonger.bib">bibtex</a>
          <p> We introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST
            collection, by applying existing state-of-the-art continual learning methods. We perform a thorough
            analysis of the performance and examine how the well-known challenges of continual learning, such as
            the catastrophic forgetting exhibit themselves in this setting.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/generative_kcl.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2112.13410">
            Generative Kernel Continual learning
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Xiantong Zhen, Ling Shao, Cees G. M. Snoek</p>
          <em>arXiv</em> <time>2021</time>
          <a target="_blank" href="data/derakhshani2021generative.bib">bibtex</a>
          <p> We introduce generative kernel continual learning, which explores and exploits the synergies
            between generative models and kernels for continual learning.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/kcl.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2210.02390">
            Kernel Continual learning
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Xiantong Zhen, Ling Shao, Cees G. M. Snoek</p>
          <em>ICML</em> <time>2021</time>
          <a target="_blank" href="data/derakhshani2021kernel.bib">bibtex</a>
          <p> This paper introduces kernel continual learning, a simple but effective variant of continual
            learning that leverages the non-parametric nature of kernel methods to tackle catastrophic
            forgetting.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" class="research__img" src="images/assisted.png" alt="method" width="160"
          height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/1906.05388">
            Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors
          </a>
          <p><strong>Mohammad Mahdi Derakhshani</strong>, Saeed Masoudnia, Amir Hossein Shaker, Omid Mersa,
            Mohammad Amin Sadeghi, Mohammad Rastegari, Babak N. Araabi</p>

          <em>CVPR</em> <time>2019</time>
          <a target="_blank" href="data/derakhshani2019assisted.bib">bibtex</a>
          <p> We present a simple and effective learning technique that significantly improves mAP of YOLO
            object detectors without compromising their speed.</p>
        </div>
      </div>

      <div class="research">
        <img class="research__img" src="images/blockcnn.png" alt="method" width="160" height="100">
        <div class="research__text">
          <a target="_blank" href="https://arxiv.org/abs/2210.02390">
            BlockCNN: A Deep Network for Artifact Removal and Image Compression
          </a>
          <p>Danial Maleki, Soheila Nadalian, Mohammad Mahdi Derakhshani, <strong>Mohammad Mahdi
              Derakhshani</strong>, Mohammad Amin Sadeghi</p>
          <em>CVPR (Workshop)</em> <time>2018</time>
          <a target="_blank" href="data/maleki2018blockcnn.bib">bibtex</a>
          <p> We present a general technique that performs both artifact removal and image compression. For
            artifact removal, we input a JPEG image and try to remove its compression artifacts.</p>
        </div>
      </div>
    </section>

    <footer class="footer">
      <p>
        Great template from <a target="_blank" href="https://github.com/mmderakhshani/mmderakhshani.github.io"
          target="_blank">
          Mohammad Mahdi
          Derakhshani
        </a>
      </p>
    </footer>
  </main>
</body>

</html>